{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: Reinforcement Learning\n",
    "### We use RatInABox to train a model-free RL agent to find a reward hidden behind a wall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"../readme_figs/rl_agent.gif\" width=\"300\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key components to most model-free RL algorithms. Policy *evaluation* evaluates a policy to learn its \"value function\". Policy *improvement* uses this value function to design a better policy. Iterating between these two often results in convergence towards a near-optimal policy. \n",
    "\n",
    "To help with these process we define a `ValueNeuron` class. The `ValueNeuron`, which is a subclass of `FeedForwardLayer` (which is a subclass of `Neurons`) recieves feedforward input from a set of input features (these will be `PlaceCells` scattered across the `Environment`) $\\{\\mathsf{\\phi}_{i}\\}_{i=1}^{N}$. The firing rate of the `ValueNeuron` is given as a weighted linear sum of its inputs:\n",
    "\n",
    "$$ \\mathsf{\\hat{V}}(\\mathsf{I}(t);\\mathsf{w}) = \\sum_{i=1}^{N}\\mathsf{w}_{i}\\mathsf{\\phi}_{i}(t).$$\n",
    "\n",
    "The goal is to adjust the weights, $\\mathsf{w}_{i}$, such that $\\mathsf{\\hat{V}}(t)$ approximates the \"value\" of the current policy. The value of the current policy $\\pi$ is defined as the decaying sum of expected future rewards\n",
    "\n",
    "$$\\mathsf{\\hat{V}}(t) \\approx \\mathsf{V}^{\\pi}(t) = \\mathbb{E} \\bigg[  \\frac{1}{\\tau} \\int_{t}^{\\infty} e^{-\\frac{t^{\\prime}-t}{\\tau}} \\mathsf{R}(t^{\\prime}) dt^{\\prime}\\bigg] $$\n",
    "\n",
    "where  the expectation is taken over any stochasticity present in the current policy (i.e. how the `Agent` moves) and `Environment` (although in this case the environment will be deterministic). This definition of value is temporally continuous: the key differences, compared to the more common form where value is written as a sum of rewards over discrete future timesteps, is that it is now a continuous integral over a reward *density* function and temporal discounting is done by the exponential decaying over a time period $\\tau$. The prefactor of $1/\\tau$ is an optional constant of normalisation. \n",
    "\n",
    "We can take the temporal derivative of this and derive a consistency equation (analagous to the Bellman equation) satisfied by this value function. This naturally gives a temporal difference-style update rule which relies on \"bootstrapping\" (the current estimate of the value function is used in lieu of the true value function) to optimize the weights of the value function approximation. A good reference for continuous RL is Doya (2000) if you want to know more about deriving this learning rule.\n",
    "\n",
    "$$ \\delta \\mathsf{w_i}(t) = \\eta \\bigg( \\mathsf{R}(t) + \\tau \\frac{d\\mathsf{\\hat{V}}(t)}{dt} - \\mathsf{\\hat{V}(t)} \\bigg) \\mathsf{e}_i(t) $$\n",
    "\n",
    "For now it suffices to observe that this learning rule is very similar to the temporally discrete TD-update rule. The first term represents the continuous analog of the temporal difference error (in fact, if you rediscretise with using the euler formula $\\dot{\\mathsf{V}}(t) = \\frac{\\mathsf{V}(t+dt) - \\mathsf{V}(t)}{dt}$ to replace the derivative, and set $dt =$ 1, you will see they are identical). The second term is the `eligibility trace' (using an eligibility trace is optional, and it could just be replaced with $\\mathsf{\\phi}_i(t)$, however doing so aids stability of the learning) of feature inputs, defined as: \n",
    "\n",
    "$$\\mathsf{e}_i(t) = \\frac{1}{\\tau_{\\mathsf{e}}} \\int_{-\\infty}^{t} e^{-\\frac{t-t^{\\prime}}{\\tau_{\\mathsf{e}}}} \\mathsf{\\phi}_i(t^{\\prime}) dt^{\\prime}$$\n",
    "\n",
    "To learn the value function, therefore, the `ValueNeuron` needs to\n",
    "1. Be able to linearly summate its inputs. Already implemented by `FeedForwardLayer.update()`\n",
    "2. Store and update the eligibility trace.\n",
    "3. Implement the learning rule. This will require access to the reward density function $\\mathsf{R}(t)$, the eligibiilty traces $\\mathsf{e}_i(t)$, the firing rate $\\mathsf{\\hat{V}}(t)$ and the temporal derivitive of the firing rate $\\frac{d\\mathsf{\\hat{V}}(t)}{dt}$\n",
    "\n",
    "We will use a temporal discounting timescale of $\\tau =$ 10 seconds and an eligibility trace timescale of $\\tau_{\\mathsf{e}} =$ 5 seconds. The input features and rewards will both be provided by layers of `RatInABox` \"`PlaceCells`\". \n",
    "\n",
    "This figure gives a summary of the network structure and the policy improvement procedure. \n",
    " <img src=\"../readme_figs/rl_networksummary.png\" width=\"750\" align=\"center\"> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import ratinabox\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ratinabox import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave this as False. \n",
    "#For paper/readme production I use a plotting library (tomplotlib) to format and save figures. Without this they will still show but not save. \n",
    "if False:\n",
    "    import tomplotlib.tomplotlib as tpl\n",
    "    tpl.figureDirectory = \"../figures/\"\n",
    "    tpl.setColorscheme(colorscheme=2)\n",
    "    save_plots = True\n",
    "else:\n",
    "    save_plots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `ValueNeuron`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNeuron(FeedForwardLayer):\n",
    "    def __init__(self, Agent, params={}):\n",
    "        default_params = {\n",
    "            \"input_layer\": None,  # the features it is using as inputs\n",
    "            \"reward_layer\": None,  # the layer which is the reward \n",
    "            \"tau\":10, #discount time horizon\n",
    "            \"tau_e\":5, #eligibility trace timescale\n",
    "            \"eta\":0.0001, #learning rate\n",
    "        }\n",
    "\n",
    "        default_params.update(params)\n",
    "        self.params = default_params\n",
    "        self.params['activation_params']={'activation':'linear'} #we use linear func approx\n",
    "        self.params['n']=1 #one value neuron \n",
    "        self.params['input_layers'] = [self.params['input_layer']]\n",
    "        super().__init__(Agent, self.params) #initialise parent class\n",
    "\n",
    "        self.et = np.zeros(params['input_layer'].n) #initialise eligibility trace\n",
    "        self.firingrate = np.zeros(1) #initialise firing rate\n",
    "        self.firingrate_deriv = np.zeros(1) #initialise firing rate derivative\n",
    "\n",
    "\n",
    "    def update_firingrate(self):\n",
    "        \"\"\"Updates firing rate as weighted linear sum of feature inputs\n",
    "        \"\"\"        \n",
    "        firingrate_last = self.firingrate \n",
    "        #update the firing rate\n",
    "        self.update() #FeedForwardLayer builtin function. this sums the inouts from the input features over the weight matrix and saves the firingrate. \n",
    "        #calculate temporal derivative of the firing rate \n",
    "        self.firingrate_deriv = (self.firingrate - firingrate_last)/self.Agent.dt\n",
    "        # update eligibility trace \n",
    "        self.et = ((self.Agent.dt/self.tau_e) * self.input_layer.firingrate + \n",
    "                   (1 - self.Agent.dt/self.tau_e) * self.et)\n",
    "        return\n",
    "\n",
    "    def update_weights(self):\n",
    "        \"\"\"Trains the weights by implementing the TD learnign rule\"\"\"\n",
    "        w = self.inputs[self.input_layer.name]['w'] #weights\n",
    "        R = self.reward_layer.firingrate #current reward\n",
    "        V = self.firingrate #current value estimate\n",
    "        dVdt = self.firingrate_deriv #currrent value derivative estimate\n",
    "        td_error = R + self.tau * dVdt - V\n",
    "        dw = self.Agent.dt*self.eta*(np.outer(td_error,self.et)) - 0.0001*w #note L2 regularisation\n",
    "        self.inputs[self.input_layer.name]['w'] += dw\n",
    "        return\n",
    "    \n",
    "    def get_steep_ascent(self,pos):\n",
    "        \"\"\"This function will be used for policy improvement. Calculates direction steepest ascent (gradient) of the value function and returns a drift velocity in this direction.\"\"\"  \n",
    "        V = self.get_state(evaluate_at=None,pos = pos)[0][0]\n",
    "        if V < 0.05*self.max_fr: #<--remeber to set max_fr on each training loop\n",
    "            return None #if the value function is too low it is unreliabe, return None\n",
    "        else: #calculate gradient locally \n",
    "            V_plusdx = self.get_state(evaluate_at=None,pos = pos+np.array([1e-3,0]))[0][0]\n",
    "            V_plusdy = self.get_state(evaluate_at=None,pos = pos+np.array([0,1e-3]))[0][0]\n",
    "            gradV = np.array([V_plusdx - V,V_plusdy-V])\n",
    "            greedy_drift_velocity = 3*self.Agent.speed_mean*gradV / np.linalg.norm(gradV)\n",
    "        return greedy_drift_velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialise an `Environment` with a wall. We put an `Agent` inside the environment. 500 `PlaceCells` of random size uniformly distributed across the environment are the input features. A single `PlaceCell` hidden behind the wall is the reward and a `ValueNeuron` (discussed above) will learn the value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Env = Environment()\n",
    "Env.add_wall([[0.8,0.0],[0.8,0.8]])\n",
    "\n",
    "Ag = Agent(Env)\n",
    "Ag.dt = 50e-3 #set discretisation time, large is fine\n",
    "Ag.episode_data = {'start_time':[],\n",
    "                   'end_time':[],\n",
    "                   'start_pos':[],\n",
    "                   'end_pos':[],\n",
    "                   'success_or_failure':[]} #a dictionary we will use later\n",
    "Ag.exploit_explore_ratio = 0.3 #a parameter we will use later\n",
    "\n",
    "n_pc = 1000\n",
    "Inputs = PlaceCells(Ag, params={'n':n_pc,\n",
    "                                 'widths':np.random.uniform(0.04,0.4,size=(n_pc)),})\n",
    "Reward = PlaceCells(Ag, params={'n':1,\n",
    "                                 'place_cell_centres':np.array([[0.9,0.05]]),\n",
    "                                 'description':'top_hat',\n",
    "                                 'widths':0.2,\n",
    "                                 'max_fr':1,})\n",
    "Reward.episode_end_time = 3 #a param we will use later\n",
    "\n",
    "ValNeur = ValueNeuron(Ag, params={'input_layer':Inputs,'reward_layer':Reward})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display the rate maps of 5 of the input features, the reward neuron and the ValueNeuron (before training with random weight initialisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Inputs.plot_rate_map(chosen_neurons='5')\n",
    "fig1, ax1 = Reward.plot_rate_map(chosen_neurons='1')\n",
    "fig2, ax2 = ValNeur.plot_rate_map(chosen_neurons='1')\n",
    "\n",
    "if save_plots == True: \n",
    "    tpl.saveFigure(fig,'RLfeatures')\n",
    "    tpl.saveFigure(fig1,'RLreward')    \n",
    "    tpl.saveFigure(fig2,'RLvalue0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a neuron capable of learning the value function under its current policy (\"policy evaluation\"). We want to use this to improve the policy (\"policy improve\") towards an optimal one. To do this will use the inferred value function to set a \"drift\" velocity which is along the direction of steepest ascent of the value function. This way the `Agent` is encouraged to move towards regions of higher and higher value. The drift velocity will be in the direction of steep ascent of the `ValueNeuron` rate map. This method of value ascent is close to a continuous analog of a \"greedy policy optimization\" in continuous action space. \n",
    "\n",
    "Learing proceeds in episode. Each episode ends when the agent gets close to the reward or times out (60 seconds). Initially the drift velocity is very weak relative to the random policy (controlled using the `drift_to_random_strength_ratio` argument) but this gets stronger as more successful episodes occur and the agent should (hopefully) converge on an approximately optimal policy. An episode ends probabilitically according to the firing rate of the reward neurons (when you are close to the reward it is likely to terminate soon) \n",
    "\n",
    "Every 5 episodes we \"cache\" a copy of the `ValueNeuron` and use this, for the next 5 episodes to provide the steepest ascent direction. We'll store details (start/end time and positions of each episode) in the `Agent` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "from copy import copy \n",
    "\n",
    "def do_episode(ref_ValNeur,\n",
    "               ValNeur,\n",
    "               Ag,\n",
    "               Inputs,\n",
    "               Reward,\n",
    "               train=True,\n",
    "               max_t=60):\n",
    "    Ag.episode_data['start_time'].append(Ag.t)\n",
    "    Ag.episode_data['start_pos'].append(Ag.pos)\n",
    "\n",
    "    ValNeur.et = np.zeros_like(ValNeur.et)\n",
    "    while True: \n",
    "        drift_velocity = ref_ValNeur.get_steep_ascent(Ag.pos)\n",
    "        #you can ignore this (force agent to travel towards reward when v nearby) helps stability. \n",
    "        if (Ag.pos[0] > 0.8)  and (Ag.pos[1] < 0.4):\n",
    "            dir_to_reward = Reward.place_cell_centres[0]-Ag.pos\n",
    "            drift_velocity = 3*Ag.speed_mean*(dir_to_reward/np.linalg.norm(dir_to_reward))\n",
    "        \n",
    "        #move the agent\n",
    "        Ag.update(drift_velocity=drift_velocity,\n",
    "                    drift_to_random_strength_ratio = Ag.exploit_explore_ratio)\n",
    "        #update inputs and train weights \n",
    "        Inputs.update()\n",
    "        Reward.update()\n",
    "        ValNeur.update_firingrate()\n",
    "        #train the weights\n",
    "        if train == True: \n",
    "            ValNeur.update_weights()\n",
    "        #end episode when at some random moment when reward is high OR after timeout \n",
    "        if np.random.uniform() < Ag.dt * Reward.firingrate/Reward.episode_end_time:\n",
    "            Ag.exploit_explore_ratio *= 1.1 #policy gets greedier if it was successful\n",
    "            Ag.episode_data['success_or_failure'].append(1)\n",
    "            break\n",
    "        if (Ag.t - Ag.episode_data['start_time'][-1]) > max_t: #timeout\n",
    "            Ag.episode_data['success_or_failure'].append(0)\n",
    "            break\n",
    "    Ag.episode_data['end_time'].append(Ag.t)\n",
    "    Ag.episode_data['end_pos'].append(Ag.pos)\n",
    "    Ag.exploit_explore_ratio = max(0.1,min(1,Ag.exploit_explore_ratio))\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets do an initial test (without learning)\n",
    "Ag.pos = np.array([0.4,0.2])\n",
    "Ag.exploit_explore_ratio = 1\n",
    "do_episode(ref_ValNeur=ValNeur,\n",
    "               ValNeur=ValNeur,\n",
    "               Ag=Ag,\n",
    "               Inputs=Inputs,\n",
    "               Reward=Reward,\n",
    "               train=False)\n",
    "fig, ax = ValNeur.plot_rate_map()\n",
    "fig, ax = Ag.plot_trajectory(fig=fig, ax=ax[0])\n",
    "if save_plots==True: \n",
    "    tpl.saveFigure(fig,\"RL_notraining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do a test over 10 batches of 8 episodes each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/10: 5 timeouts, 3 successes, average episode time 46.35s\n",
      "Batch 2/10: 5 timeouts, 3 successes, average episode time 51.46s\n",
      "Batch 3/10: 2 timeouts, 6 successes, average episode time 29.22s\n",
      "Batch 4/10: 0 timeouts, 8 successes, average episode time 10.10s\n",
      "Batch 5/10: 0 timeouts, 8 successes, average episode time 14.19s\n",
      "Batch 6/10: 0 timeouts, 8 successes, average episode time 11.04s\n",
      "Batch 7/10: 0 timeouts, 8 successes, average episode time 11.87s\n",
      "Batch 8/10: 0 timeouts, 8 successes, average episode time 10.96s\n",
      "Batch 9/10: 0 timeouts, 8 successes, average episode time 10.44s\n",
      "Batch 10/10: 0 timeouts, 8 successes, average episode time 12.77s\n"
     ]
    }
   ],
   "source": [
    "Ag.exploit_explore_ratio = 0.3\n",
    "for i in range(10):\n",
    "    #cache copy of the ValueNeuron and use this to dictate policy\n",
    "    ref_ValNeur = copy(ValNeur)\n",
    "    ref_ValNeur.max_fr = np.max(ValNeur.get_state(evaluate_at='all'))\n",
    "\n",
    "    for j in range(8): #batches of episodes \n",
    "        Ag.pos = Env.sample_positions(n=1)[0] #put agent in random position\n",
    "        do_episode(ref_ValNeur,\n",
    "                   ValNeur,\n",
    "                   Ag,\n",
    "                   Inputs,\n",
    "                   Reward,\n",
    "                   train=True)\n",
    "\n",
    "    n_success = sum(Ag.episode_data['success_or_failure'][-8:])\n",
    "    av_episode_time = np.mean(np.array(Ag.episode_data['end_time'][-8:]) - np.array(Ag.episode_data['start_time'][-8:]))\n",
    "    print(f\"Batch {i+1}/{10}: {8-n_success} timeouts, {n_success} successes, average episode time {av_episode_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a pretty figure. We'll start the agent off at 8 evenly spaced locations across the environment and run an apisode from each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = np.array([[0.1,0.9],\n",
    "                     [0.1,0.7],\n",
    "                     [0.1,0.5],\n",
    "                     [0.1,0.3],\n",
    "                     [0.1,0.1],\n",
    "                     [0.3,0.1],\n",
    "                     [0.5,0.1],\n",
    "                     [0.7,0.1],])\n",
    "test_pos += np.random.uniform(-0.05,0.05,size=test_pos.shape)\n",
    "np.random.shuffle(test_pos)\n",
    "Env.walls[-1,-1,-1]=0.8\n",
    "Reward.episode_end_time = 1\n",
    "for j in range(8):\n",
    "    Ag.pos = test_pos[j]\n",
    "    do_episode(ref_ValNeur,\n",
    "               ValNeur,\n",
    "               Ag,\n",
    "               Inputs,\n",
    "               Reward,\n",
    "               train=False)\n",
    "n_success = sum(Ag.episode_data['success_or_failure'][-8:])\n",
    "av_episode_time = np.mean(np.array(Ag.episode_data['end_time'][-8:]) - np.array(Ag.episode_data['start_time'][-8:]))\n",
    "print(f\"Batch {i+1}/{10}: {8-n_success} timeouts, {n_success} successes, average episode time {av_episode_time:.2f}s\")\n",
    "\n",
    "Ag.average_measured_speed = 0.15\n",
    "fig, ax = ValNeur.plot_rate_map()\n",
    "fig, ax = Ag.plot_trajectory(fig=fig,ax=ax[0],t_start=Ag.episode_data['start_time'][-8]+Ag.dt)\n",
    "start_pos = np.array(Ag.episode_data['start_pos'][-8:])\n",
    "end_pos = np.array(Ag.episode_data['end_pos'][-8:])\n",
    "ax.scatter(start_pos[:,0],start_pos[:,1],s=20,c='C2',zorder=11,alpha=0.8,linewidths=0)\n",
    "ax.scatter(end_pos[-8:,0],end_pos[-8:,1],s=20,c='r',zorder=11,alpha=0.8,linewidths=0)\n",
    "if save_plots == True: \n",
    "    tpl.saveFigure(fig,'RL_trainedagent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Freebie from doing RL in continuous space and time\n",
    "\n",
    "One of the key premises leading to the development of `RatInABox` is that, oftentimes, studying systems in their full form -- i.e. using motion models which are continuous in time and state encodings which are continuous in space and interact adaptively with the environment -- can lead to results which are quantitatively different from gridworld models. These might explain aspects of behaviour inaccessible using discretised models. What might these be? \n",
    "\n",
    "As an example, suppose a shortcut was made avaiable near the bottom of the wall. In this case the place cells near the reward location might be expected to 'bleed' out. This occurs naturally in `RatInABox`: since `PlaceCell` firing rates are calculated on-the-fly (not cached) any change to the environment immediately effects the place cells. Since the `ValueNeuron` receptive field is just a linear sum of these place cells as the place cells bleed out through the hole in the wall the inferred value of this area increases. The policy (gradient ascent of the value function) may then follow this and automatically find a `shortcut' to the reward with zero extra training. Let's find out: \n",
    "\n",
    "Here we make a shortcut available by manually altering the wall structure of the environment. Everythign else is taken care of by the software (i.e. input `PlaceCell` firing fields, and the `ValueNeuron` firing will all automatically respect this new change in the environment). *No extra training is performed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITHOUT SHORTCUT\n",
    "Env.walls[-1] = np.array([[0.8,0.0],[0.8,0.8]])\n",
    "Reward.episode_end_time = 3\n",
    "Ag.pos = np.array([0.4,0.2])\n",
    "do_episode(ref_ValNeur,\n",
    "            ValNeur,\n",
    "            Ag,\n",
    "            Inputs,\n",
    "            Reward,\n",
    "            train=False)\n",
    "\n",
    "Ag.average_measured_speed = 0.15\n",
    "fig, ax = ValNeur.plot_rate_map()\n",
    "fig, ax = Ag.plot_trajectory(fig=fig,ax=ax[0],t_start=Ag.episode_data['start_time'][-1]+Ag.dt)\n",
    "\n",
    "if save_plots == True:\n",
    "    tpl.saveFigure(fig,\"rl_noshortcut\")\n",
    "    anim = Ag.animate_trajectory(t_start = Ag.episode_data['start_time'][-1]+Ag.dt, t_end=Ag.history['t'][-1],speed_up=1)\n",
    "    anim.save(\"../figures/animations/rl_agent_noshortcut.mp4\",dpi=250)\n",
    "\n",
    "\n",
    "#WITH SHORTCUT \n",
    "Env.walls[-1] = np.array([[0.8,0.1],[0.8,0.8]])\n",
    "Reward.episode_end_time = 3\n",
    "Ag.pos = np.array([0.4,0.2])\n",
    "do_episode(ref_ValNeur,\n",
    "            ValNeur,\n",
    "            Ag,\n",
    "            Inputs,\n",
    "            Reward,\n",
    "            train=False)\n",
    "Ag.average_measured_speed = 0.15\n",
    "fig1, ax1 = ValNeur.plot_rate_map()\n",
    "fig1, ax1 = Ag.plot_trajectory(fig=fig1,ax=ax1[0],t_start=Ag.episode_data['start_time'][-1]+Ag.dt)\n",
    "\n",
    "if save_plots == True:\n",
    "    tpl.saveFigure(fig1,\"rl_shortcut\")\n",
    "    anim = Ag.animate_trajectory(t_start = Ag.episode_data['start_time'][-1]+Ag.dt, t_end=Ag.history['t'][-1],speed_up=1)\n",
    "    anim.save(\"../figures/animations/rl_agent_shortcut.mp4\",dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer: \n",
    "Please note this script is intended as a tutorial, here only to demonstrate of how `RatInABox` could be used for a reinforcement learning project in continuous time and space and not, in any way, as a scientific result regarding the role of hippocampus' role in navigational reinforcement learning. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3cdbb439150f468ef73eca921e462ba1cf0894f575003150b1b7f3566c8fd4a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
